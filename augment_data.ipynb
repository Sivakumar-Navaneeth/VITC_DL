{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4170 images belonging to 58 classes.\n",
      "Found 2003 images belonging to 58 classes.\n"
     ]
    }
   ],
   "source": [
    "labels_path = \"datasets/traffic_sign/labels.csv\"\n",
    "classes = pd.read_csv(labels_path)\n",
    "class_names = list(classes['Name'])\n",
    "num_classes = len(class_names)\n",
    "data_dir = \"datasets/traffic_sign/traffic_Data\"\n",
    "\n",
    "# Load the class labels\n",
    "class_labels = pd.read_csv(labels_path)\n",
    "class_names = list(class_labels['Name'])\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Data Preprocessing and Augmentation\n",
    "batch_size = 32\n",
    "image_size = (75, 75)  # Minimum input size required by InceptionV3\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=lambda x: tf.image.resize(x, image_size)\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_datagen = datagen.flow_from_directory(\n",
    "    data_dir+\"/DATA\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_datagen = datagen.flow_from_directory(\n",
    "    data_dir+\"/TEST\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    # subset='validation' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "original_data_dir = r\"datasets\\traffic_sign\\traffic_Data\\DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths to the original dataset and augmented data directory\n",
    "original_data_dir = r\"datasets\\traffic_sign\\traffic_Data\\DATA\"\n",
    "augmented_data_dir = r\"datasets\\traffic_sign\\traffic_Data\\DATA_AUG\"\n",
    "\n",
    "# Create the augmented data directory if it does not exist\n",
    "os.makedirs(augmented_data_dir, exist_ok=True)\n",
    "\n",
    "# Perform data augmentation and save the augmented images to the augmented data directory\n",
    "for root, dirs, files in os.walk(original_data_dir):\n",
    "    for file in files:\n",
    "        image_path = os.path.join(root, file)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Horizontal flip\n",
    "        flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flipped_image.save(os.path.join(augmented_data_dir, \"flipped_\" + file))\n",
    "\n",
    "        # Vertical flip\n",
    "        flipped_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        flipped_image.save(os.path.join(augmented_data_dir, \"flipped_\" + file))\n",
    "\n",
    "        # Center crop\n",
    "        width, height = image.size\n",
    "        crop_size = min(width, height)\n",
    "        left = (width - crop_size) // 2\n",
    "        top = (height - crop_size) // 2\n",
    "        right = left + crop_size\n",
    "        bottom = top + crop_size\n",
    "        cropped_image = image.crop((left, top, right, bottom))\n",
    "        cropped_image.save(os.path.join(augmented_data_dir, \"cropped_\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the augmented data files with the original dataset\n",
    "for root, dirs, files in os.walk(augmented_data_dir):\n",
    "    for file in files:\n",
    "        shutil.move(os.path.join(root, file), os.path.join(original_data_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4170 images belonging to 58 classes.\n",
      "Statistics of the dataset before augmentation:\n",
      "Dataset size: 4170\n",
      "Number of classes: 58\n",
      "\n",
      "Statistics of the dataset after augmentation:\n",
      "Dataset size: 12510\n",
      "Number of classes: 58\n"
     ]
    }
   ],
   "source": [
    "# Update the class labels and number of classes\n",
    "labels_path = r\"datasets\\traffic_sign\\labels.csv\"\n",
    "class_labels = pd.read_csv(labels_path)\n",
    "# class_names = list(class_labels['Name'])\n",
    "class_names = [str(i).zfill(3) for i in range(num_classes)]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Update the training dataset\n",
    "train_datagen = datagen.flow_from_directory(\n",
    "    original_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Calculate the statistics of the dataset before augmentation\n",
    "original_dataset_size = len(train_datagen.filenames)\n",
    "original_num_classes = len(train_datagen.class_indices)\n",
    "original_class_counts = train_datagen.classes\n",
    "\n",
    "# Calculate the statistics of the dataset after augmentation\n",
    "augmented_dataset_size = len(os.listdir(original_data_dir)) - original_num_classes + original_dataset_size\n",
    "augmented_num_classes = len(class_labels)\n",
    "augmented_class_counts = [len(os.listdir(original_data_dir+\"\\\\\"+class_name)) for class_name in class_names]\n",
    "\n",
    "# Print the statistics of the dataset before and after augmentation\n",
    "print(\"Statistics of the dataset before augmentation:\")\n",
    "print(\"Dataset size:\", original_dataset_size)\n",
    "print(\"Number of classes:\", original_num_classes)\n",
    "# print(\"Class counts:\", original_class_counts)\n",
    "\n",
    "print(\"\\nStatistics of the dataset after augmentation:\")\n",
    "print(\"Dataset size:\", augmented_dataset_size)\n",
    "print(\"Number of classes:\", augmented_num_classes)\n",
    "# print(\"Class counts:\", augmented_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testbed_69",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
